{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d566992-4270-4195-a001-98fd664875fe",
   "metadata": {},
   "source": [
    "<p style=\"text-align:center\">\n",
    "    <a href=\"https://www.ict.mahidol.ac.th/en/\" target=\"_blank\">\n",
    "    <img src=\"https://www3.ict.mahidol.ac.th/ICTSurveysV2/Content/image/MUICT2.png\" width=\"400\" alt=\"Faculty of ICT\">\n",
    "    </a>\n",
    "</p>\n",
    "\n",
    "# Lab07: ML Basics: Regression\n",
    "\n",
    "This Lab Assignment is an exercise in Regression modelling using a more sophisticated model to predict and understand house prices in the Melbourne dataset (2017). The assignment will guide you to train a more advanced model on the pricing dataset, report and compare its error scores, and view its learning curves. Furthermore, the assignment will take you through steps to export the model and make predictions within a web dashboard.\n",
    "\n",
    "__Learning Objectives are:__\n",
    "* Recognize Overfitting & Underfitting.\n",
    "* Compare Models with Error Metrics (MAE, R2)\n",
    "* Deliver Predictions in a Dashboard\n",
    "* Inform users that model predictions come with errors: **f(x) = y_pred + error + noise**\n",
    "\n",
    "\n",
    "__Instructions:__\n",
    "1. Append your ID at the end of this jupyter file name. For example, ```ITCS227_Lab07_Assignment_6788123.ipynb```\n",
    "2. Complete each task in the lab.\n",
    "3. Once finished, raise your hand to call a TA.\n",
    "4. The TA will check your work and give you an appropriate score.\n",
    "5. Submit your IPYNB source code to MyCourse as record-keeping."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63109e7-283d-4298-86d3-bb3a05259028",
   "metadata": {},
   "source": [
    "## 0. Install Packages:\n",
    "- Uncomment this line to install the `Streamlit` dashboard library, we will use later.\n",
    "\n",
    "*If your machine reports other missing libraries, then you may need to install the library using pip. (Mostly the libraries used are already installed with Anaconda).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a38fe5-4ca5-4e29-8f2e-692a3489488f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install streamlit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf2ebef1-0161-4907-83ed-bcb2e406572d",
   "metadata": {},
   "source": [
    "# Explaining Melbourne Housing Market - "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a1abbf-4817-44d0-a040-925b47be7f76",
   "metadata": {},
   "source": [
    "### Dataset Source: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9174356c-b5f6-45e9-a3ba-d16b94ee7d69",
   "metadata": {},
   "source": [
    "Melbourne real estate is BOOMING. Can you find the insight or predict the next big trend to become a real estate mogul… or even harder, to snap up a reasonably priced 2-bedroom unit?\n",
    "\n",
    "**Content**\n",
    "- This is a snapshot of a dataset created by Tony Pino.\n",
    "- It was scraped from publicly available results posted every week from Domain.com.au. He cleaned it well, and now it's up to you to make data analysis magic. The dataset includes Address, Type of Real estate, Suburb, Method of Selling, Rooms, Price, Real Estate Agent, Date of Sale and distance from C.B.D.\n",
    "\n",
    "**Notes on Specific Variables**\n",
    "- Rooms: Number of rooms\n",
    "- Price: Price in dollars\n",
    "- Method: S - property sold; SP - property sold prior; PI - property passed in; PN - sold prior not disclosed; SN - sold not disclosed; NB - no bid; VB - vendor bid; W - withdrawn prior to auction; SA - sold after auction; SS - sold after auction price not disclosed. N/A - price or highest bid not available.\n",
    "- Type: br - bedroom(s); h - house,cottage,villa, semi,terrace; u - unit, duplex; t - townhouse; dev site - development site; o res - other residential.\n",
    "- SellerG: Real Estate Agent\n",
    "- Date: Date sold\n",
    "- Distance: Distance from CBD\n",
    "- Regionname: General Region (West, North West, North, North east …etc)\n",
    "- Propertycount: Number of properties that exist in the suburb.\n",
    "- Bedroom2 : Scraped # of Bedrooms (from different source)\n",
    "- Bathroom: Number of Bathrooms\n",
    "- Car: Number of carspots\n",
    "- Landsize: Land Size\n",
    "- BuildingArea: Building Size\n",
    "- CouncilArea: Governing council for the area\n",
    "\n",
    "**Acknowledgements**\n",
    "- This is intended as a static (unchanging) snapshot of https://www.kaggle.com/anthonypino/melbourne-housing-market. It was created in September 2017. Additionally, homes with no Price have been removed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bea84cd-aca0-482d-995e-9e723eda0a17",
   "metadata": {},
   "source": [
    "### Load Dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249d71af-473f-4f4c-9402-245f22ef0bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('files/melb_data.csv').drop('Unnamed: 0',axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0152026-b649-4c57-8141-b93359ccd805",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Visualize the Dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "057fb8ef-b76b-4ef9-b47d-adab99c81916",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "\n",
    "def plotly_map(df, latlng_cols=('lat','lng'), z=None, custom_data_cols=[], custom_text=[], center_dict=dict(lat=13.6, lon=100.4), zoom=5, WRITE=False, WRITE_FN=None):\n",
    "    \"\"\" \n",
    "    @WRITE_FN - do not include extension - i.e. `.png` or `.html`, as both files will be written.\n",
    "    Docs:   https://plotly.com/python-api-reference/generated/plotly.express.density_mapbox.html\n",
    "            https://plotly.com/python/mapbox-density-heatmaps/\n",
    "    \"\"\"\n",
    "    pio.templates.default = 'plotly_white' # 'plotly_dark'\n",
    "    fig = px.density_mapbox(df, \n",
    "                            lat=latlng_cols[0], \n",
    "                            lon=latlng_cols[1], \n",
    "                            z=z,\n",
    "                            radius=5,\n",
    "                            center=center_dict, zoom=zoom,\n",
    "                            mapbox_style=[\"open-street-map\",'carto-darkmatter'][0],\n",
    "                            custom_data=custom_data_cols,\n",
    "                           )\n",
    "\n",
    "    fig.update_layout(margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0})\n",
    "    if custom_text:\n",
    "        fig.update_traces(\n",
    "            hovertemplate=\"<br>\".join(custom_text)\n",
    "        )\n",
    "    fig.show(config={'displayModeBar': False} )\n",
    "    if WRITE:\n",
    "        if WRITE_FN != None and isinstance(WRITE_FN, str) and len(WRITE_FN)>4:\n",
    "            ofn = f'{WRITE_FN}_MapPlot_{TIMESTAMP_FILENAME()}'\n",
    "            fig.write_image(ofn+'.png')\n",
    "            fig.write_html(ofn+'.html', full_html=False, include_plotlyjs=False, include_mathjax=False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27f8b17-83a3-4d61-899a-b1ee614e56b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfm = df.dropna()\n",
    "plotly_map(dfm[dfm['Method']=='S'], \n",
    "           latlng_cols=('Lattitude','Longtitude'), \n",
    "           z='Price',\n",
    "           custom_data_cols=['CouncilArea',\n",
    "                             'Distance',\n",
    "                             'Landsize',\n",
    "                             'BuildingArea',\n",
    "                             'Rooms',\n",
    "                             'Bathroom',\n",
    "                             'Price'\n",
    "                           ], \n",
    "           custom_text=['Area: %{customdata[0]}',\n",
    "                        'Distance: %{customdata[1]}',\n",
    "                        'LS / BA: %{customdata[2]}/%{customdata[3]}',\n",
    "                        'Rm / Br: %{customdata[4]}/%{customdata[5]}',\n",
    "                        'Price AUD-$: %{customdata[6]:.,1f}'\n",
    "                            ],\n",
    "           center_dict=dict(lat=-37.814, lon=144.963),\n",
    "           zoom=9\n",
    "          )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268e2a34-26a1-4d61-8637-b2f657ad7cd1",
   "metadata": {},
   "source": [
    "### Display Dataset Features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83538961-c0e8-48f7-b942-3b7843685728",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total Num of Records ',len(df))\n",
    "display( df.dtypes )\n",
    "display( df.head(2) )\n",
    "display( df.describe().T )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017a7d32-407c-452a-ad6d-8d14863d9708",
   "metadata": {},
   "source": [
    "## Collect the Dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727eb056-dd5b-478a-be61-84b13eb248c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "def get_some_data(data, features, target='Price'):\n",
    "    _data = data.dropna()\n",
    "    X = _data[features]\n",
    "    y = _data[target]\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48af0b5f-4d07-4174-8ab5-ecb8833ac8d9",
   "metadata": {},
   "source": [
    "# Choose Features, Model & Measure Errors:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4511856d-a4df-43f8-bbbc-dfc3fe704c90",
   "metadata": {},
   "source": [
    "## Q1: - Choose the Input Features (Columns) for `X` from the `df` Dataset to Predict House Prices `Y`:\n",
    "\n",
    "* To learn the model, we need define the `X` and the `Y`.\n",
    "* For `X` - we started with `['Distance']`.\n",
    "* You can choose from your `df` (DataFrame above).\n",
    "* For `Y` - use the `Price` (as we did earlier)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1438fabe-1513-4edd-a2cf-e13306a817b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert your code here\n",
    "features = [ ]\n",
    "# X, y = get_some_data( df, features )\n",
    "print('Number of Features: ', len(features))\n",
    "print('Features Names: ', features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f480e11-dfe0-409f-be6a-5442076acd70",
   "metadata": {},
   "source": [
    "* If you've selected `Categorical` features, uncomment the following to convert them to numeric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5cab0a-2389-425f-8bab-ced1c9f5ba33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------ Uncomment this section if you choose to use Categorical / String / Object features:\n",
    "# ------------ This will encode the Strings as Integers.\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "# import json\n",
    "# \n",
    "# def export_mapping_to_json(mapping:dict):\n",
    "#     def convert_numpy(obj): # Recursive function to handle nested structures\n",
    "#         if isinstance(obj, np.integer): return int(obj)\n",
    "#         elif isinstance(obj, np.floating): return float(obj)\n",
    "#         elif isinstance(obj, np.ndarray): return obj.tolist()\n",
    "#         elif isinstance(obj, dict): return {k: convert_numpy(v) for k, v in obj.items()}\n",
    "#         elif isinstance(obj, list) or isinstance(obj, tuple): return [convert_numpy(elem) for elem in obj]\n",
    "#         else: return obj\n",
    "#     ofn = 'x_categorical_to_integer_mapping.json'\n",
    "#     with open(ofn, 'w') as f:\n",
    "#         json.dump(convert_numpy(mapping), f, indent=4)\n",
    "#         print(f\"Mapping successfully exported to {ofn}\")\n",
    "# \n",
    "# def encode_object_to_integers( X ):\n",
    "#     mapping = {}\n",
    "#     le = LabelEncoder()\n",
    "#     for col in X.select_dtypes(include=['object']).columns:\n",
    "#         X[col] = le.fit_transform(X[col])\n",
    "#         mapping[col] = dict(zip(le.classes_, le.transform(le.classes_)))\n",
    "#     export_mapping_to_json(mapping)\n",
    "#\n",
    "# encode_object_to_integers( X )\n",
    "# ------------ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cedec0b-2ba7-45f8-bb9f-743d7d415834",
   "metadata": {},
   "source": [
    "## Q2: - Split the X and Y into Train and Test sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499dd4f8-7296-4edc-84e7-4d92467c1ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# insert your code here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ddf4a63-22cf-4f8d-a374-2e51ca75e868",
   "metadata": {},
   "source": [
    "<details><summary><span style=\"color:red\">&#x1F6C8; Help</span> (Use this only as a last resort!!)</summary>\n",
    "\n",
    "```Python\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=0) # Split into 85% training and 15% testing data\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5305c09-e875-4d13-a5b2-ca519625f6f3",
   "metadata": {},
   "source": [
    "## Q3: - Choose a Regression Model Algorithm to Fit:\n",
    "\n",
    "1. Find a regression model to try out.\n",
    "2. Add the import statement for the model.\n",
    "3. Instantiate the model.\n",
    "4. Call the `fit( ... )` function on the model, as we did in Tutorials 06 & 07."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc571cb-3e38-473e-8235-72e58bd8d2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import * \n",
    "from sklearn.ensemble import *\n",
    "# insert your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eabfed00-dcdd-453c-bedb-eee9fafac990",
   "metadata": {},
   "source": [
    "<details><summary><span style=\"color:#17c1e5\">&#x1F6C8; Hint</span> (Extra info)</summary>\n",
    "\n",
    "* You can take a read of the [Scikit-Learn Documentation](https://scikit-learn.org/stable/modules/ensemble.html) which have several good choices of `regression models` for tabular datasets (both numeric and categorical features).\n",
    "* You might ask Mahidol's Gemini Account to give you some ideas. It has to be a `regression model` to predict a single number.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b59fb5-d557-4ce6-91f8-b1eb9e33fc8f",
   "metadata": {},
   "source": [
    "<details><summary><span style=\"color:red\">&#x1F6C8; Help</span> (Use this only as a last resort!!)</summary>\n",
    "\n",
    "* We can use the `HistGradientBoostingRegressor` which is an excellent model for tabular data, for regression (and for classification using `HistGradientBoostingClassifier`).\n",
    "> * This model builds an \"[ensemble](https://scikit-learn.org/stable/modules/ensemble.html)\" of `100` \"decision trees\" (a flowchart) and iteratively improves its decisions (accuracy) to reduce its errors.\n",
    "> * The collection of all trees gives the most balanced, least error result, which is a weighted combination of all trees.\n",
    "> * Like all Ensembles, GBR is designed to manage error improvements on both train and test sets.\n",
    "> \n",
    "> **Reference:**  Friedman, J.H. (2001). Greedy function approximation: A gradient boosting machine. Annals of Statistics, 29, 1189-1232.\n",
    "\n",
    "* There are more recent and more advanced models. This one is simple to use and gives good results. It will automatically encode categorical items (similar to one-hot encoding), and handles numeric value as we will see.\n",
    "\n",
    "```python\n",
    "my_model = HistGradientBoostingRegressor()\n",
    "my_model.fit( X_train, y_train )\n",
    "```\n",
    "* or you can try:\n",
    "```python\n",
    "my_model = RandomForestRegressor()\n",
    "my_model.fit( X_train, y_train )\n",
    "```\n",
    "* or you can try:\n",
    "```python\n",
    "my_model = LinearRegression()\n",
    "my_model.fit( X_train, y_train )\n",
    "\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "136205f1-6e1e-444d-96aa-c620cf8c75db",
   "metadata": {},
   "source": [
    "## Q4: - Measure the MAE and R2 errors for the `test` set of this new model:\n",
    "1. Get the `y_pred` prediction on the `test set`:\n",
    "2. Measure and show their MAE score for this model.\n",
    "3. Measure and show their R2 scores for this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2ba13b-8774-4d1c-8e69-db8216cf0127",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "# insert your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "921408b6-aae6-4437-b715-0f42feb9db49",
   "metadata": {},
   "source": [
    "<details><summary><span style=\"color:red\">&#x1F6C8; Help</span> (Use this only as a last resort!!)</summary>\n",
    "\n",
    "```Python\n",
    "y_pred = my_model.predict(X_test)\n",
    "print(f\"Test: mean_absolute_error: {mean_absolute_error( ... , y_pred):,.2f}\")\n",
    "print(f\"Test: r2_score: {r2_score(  ...  , y_pred):,.2f}\")\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "966c629d-e048-494f-88e5-8c80af0e3f95",
   "metadata": {},
   "source": [
    "## Q5: - Measure the MAE and R2 errors for the `train` set of this new model:\n",
    "1. Get the `y_pred` prediction  on the `train set`:\n",
    "2. Measure and show their MAE score for this model.\n",
    "3. Measure and show their R2 scores for this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2718022e-c10e-45eb-818d-9b4fc1b1f8aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04cd5b56-78c5-4628-a0de-f7cf531ea863",
   "metadata": {},
   "source": [
    "<details><summary><span style=\"color:red\">&#x1F6C8; Help</span> (Use this only as a last resort!!)</summary>\n",
    "\n",
    "```Python\n",
    "print('Similar to above, but you will use `X_train` and `y_train`.')\n",
    "print('This is because we will use on the `training set`, instead of the `test set`.')\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac25c50-5ae7-46f8-84d4-cfcc7aa266f2",
   "metadata": {},
   "source": [
    "# Making a Learning curve plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b1fc4ec-89d8-4408-a217-3a020c3e8f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def _plot_learning_curve(estimator, train_sizes, train_scores, valid_scores, metric='MAE', plt_text='', ax=None):\n",
    "    train_errors = -train_scores.mean(axis=1)\n",
    "    valid_errors = -valid_scores.mean(axis=1)\n",
    "    ax = plt.gca() if not ax else ax\n",
    "    ax.plot(train_sizes, train_errors, \"r-+\", linewidth=2, label=\"train\")\n",
    "    ax.plot(train_sizes, valid_errors, \"b-\", linewidth=3, label=\"valid\")\n",
    "    ax.set_xlabel(\"Training set size\")\n",
    "    ax.set_ylabel(f'{metric}')\n",
    "    # plt.gca().set_xscale(\"log\", nonpositive='clip')\n",
    "    ax.grid()\n",
    "    ax.legend(loc=\"upper right\")\n",
    "    ax.set_ylim(bottom=0, top=1.25*max([max(train_errors), max(valid_errors)]))\n",
    "    ax.set_title(f'{estimator.__class__.__name__}\\n{plt_text}', fontsize=8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c115ee2-05ec-4271-bb2c-87fe10bd7fc3",
   "metadata": {},
   "source": [
    "## Q6: - Show the learning curve plots of this model:\n",
    "1. Using the function provided,  create the `learning curve` Plots for your new model.\n",
    "\n",
    "*NB: As your new model is (probaly) more complex than the LinearRegression, it can take longer to perform this function. If so, reduce the 10 and 5 values to smaller numbers.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df104f56-e015-45a8-b8ca-bb661e6de122",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import learning_curve\n",
    "# insert your code here.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cfcb95b-bcef-471e-90df-cd8720dd915b",
   "metadata": {},
   "source": [
    "<details><summary><span style=\"color:#17c1e5\">&#x1F6C8; Hint</span> (Extra info)</summary>\n",
    "\n",
    "* First call the `.. = learning_curve(..)` function. Examples in the Tutorial.ipynb.\n",
    "* Second use the plot function above.\n",
    "```Python\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(15, 3), sharey=True, sharex=True)\n",
    "_plot_learning_curve(my_model, train_sizes, train_scores, valid_scores, metric='MAE', plt_text='', ax=None)\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10f99a7-71ea-458b-ba00-9f0672ec2781",
   "metadata": {},
   "source": [
    "<details><summary><span style=\"color:red\">&#x1F6C8; Help</span> (Use this only as a last resort!!)</summary>\n",
    "\n",
    "```Python\n",
    "train_sizes, train_scores, valid_scores = learning_curve( my_model ,\n",
    "                    X_train , y_train ,\n",
    "                    train_sizes=np.linspace(0.05, 1.0, 10), # 10 size intervals, from 5% to 100%\n",
    "                    cv=5,     # CV=5 means  Train = 80%  , Valid = 20%.\n",
    "                              # CV=10 means Train = 90%  , Valid = 10%.\n",
    "                              #   - The fit/predict is repeated 5 times with random samples taken from X/Y.\n",
    "                              #   - The resulting error is the average across all 5 trials; so a smoother and fairer result than CV=1 , which is hold-out.\n",
    "                    scoring=\"neg_mean_absolute_error\"\n",
    "                )\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(15, 3), sharey=True, sharex=True)\n",
    "_plot_learning_curve(my_model, train_sizes, train_scores, valid_scores, metric='MAE', plt_text='', ax=None)\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d098c6-62aa-45dd-b98b-754980d9d4b6",
   "metadata": {},
   "source": [
    "## Q7: - Write an explanation for your model's learning curve:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430dc5fa-8dc6-4e0d-b1a7-2eb8ee27ab8b",
   "metadata": {},
   "source": [
    "1. According to the learning curve, is the model `overfitted` or `underfitted` or has a `generalized fit`? Explain your decision.\n",
    "\n",
    "Your answer: \n",
    "- `[type your answer]`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44de28a-bf70-4986-bdef-ed2ca06b9dac",
   "metadata": {},
   "source": [
    "<details><summary><span style=\"color:red\">&#x1F6C8; Help</span> (Use this only as a last resort!!)</summary>\n",
    "\n",
    "\n",
    "> Refer to the slides for pictures of different fits.\n",
    "\n",
    "> * Most model algorithms will give a `\"GENERALIZED FIT\"` by default. i.e. both train and valid error scores are close together.\n",
    "> * If train and valid error scores are close together, but the MAE error is high (like 300,000 Dollars!), then we might consider this `\"UNDERFIT\"`. - Underfitting both training and testing data.\n",
    "> * If Test/Valid Error is high, but Train Error is low, then this is `\"OVERFIT\"` - overfitting the training data.\n",
    "> \n",
    "> The difference between `GENERALIZED FIT` and `UNDERFIT`, might depend on what is a good error level.\n",
    "> \n",
    "> For example:\n",
    "> * We might decide that 50,000 AUD is good (MAE) error.\n",
    ">    * If MAE is > 50,000 AUD, then we might say the model is still `\"UNDERFITTING\"`.\n",
    "> \n",
    "> * Or, we might decide that R2 > 0.8 is good.\n",
    ">    * If R2 is < 0.8, then perhaps the model is still `\"UNDERFITTING\"`.\n",
    "> \n",
    "> You can decide what than you would like (r2=1.0 = 100% is perfect! as is MAE = $0 dollars). There's a human decision here to make :-)\n",
    "> \n",
    "> If `UNDERFITTING`, we might want to take more model improvement steps: including more features, feature representations or a more complex model.\n",
    "> \n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4129d645-82ac-4dfc-9c1f-19ed96fa03cd",
   "metadata": {},
   "source": [
    "## Q8: - Compare Models: - Write an explanation for your model's errors:\n",
    "\n",
    "Answer the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9837f590-cb7e-4ef0-a28f-a481f8f80b39",
   "metadata": {},
   "source": [
    "From Lab_07_Tutorial and above, find and fill-in the following information for the `Test Set`:\n",
    "\n",
    "1. `Polynomial Degree-2` model's `MAE Score` was: ________\n",
    "2. `Polynomial Degree-2` model's `R2 Score` was: ________\n",
    "3. `My_Model` model's `MAE Score` was: ________\n",
    "4. `My_Model` model's `R2 Score` was: ________\n",
    "5. Is the `MAE error` and the `R2 score` better or worse?\n",
    "    - Is `My_Model` Better / Worse?:  _________\n",
    "    - What is the `MAE (AUD $)` difference between models?:  _________\n",
    "    - What is the `R2 Score` difference between models?:  _________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0c6d6a-5c50-4643-8ad6-a7882000597f",
   "metadata": {},
   "source": [
    "<details><summary><span style=\"color:red\">&#x1F6C8; Help</span> (Use this only as a last resort!!)</summary>\n",
    "\n",
    "```\n",
    "Check your solutions above and Lab07_Tutorial for specific answers.\n",
    "The Polynomial Degree-2 model's:\n",
    "- MAE Score is ~ 323,713.60.\n",
    "- R2 Score is ~ 0.43.\n",
    "The My_Model model's:\n",
    "- MAE Score is ~ ????????\n",
    "- R2 Score is ~ ???\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91fe0ff7-81e0-49de-82fb-5d7c7102d13c",
   "metadata": {},
   "source": [
    "# Export your model to a file:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295479ee-6d13-426f-87f4-0820a9c71511",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import pickle\n",
    "\n",
    "def export_model_to_file( model ):\n",
    "    print('Saving files to directory: ', os.getcwd())\n",
    "    model_name = f'x_Exported_Model_housing-model'\n",
    "    ofn1 = f'{model_name}.pkl'\n",
    "    with open(ofn1, 'wb') as file:\n",
    "        pickle.dump(model, file)\n",
    "        print(f'Exported to file:\\t {ofn1}')\n",
    "    model_bytes = pickle.dumps(model)\n",
    "    model_str = model_bytes.decode('latin1')\n",
    "    ofn2 = f'{model_name}.txt'\n",
    "    with open(ofn2, 'w') as file:\n",
    "        file.write( '\\n'.join( [\n",
    "            f'The {model.__class__.__name__} Model is {len(model_str):,} chars in total, at (approx.) memory size: {sys.getsizeof(model_bytes)/1024:,.2f} KiB\\n\\n', \n",
    "        ]))\n",
    "        print(f'Exported to file:\\t {ofn2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3223906a-a757-45fd-9ed4-bcb8fc164b36",
   "metadata": {},
   "source": [
    "## Q9: - Export your trained model as a file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce1cb1f-525c-4032-82f4-e4e4f024906a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert your code here.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8edce73-fc9f-484e-988f-1ea7923c95b6",
   "metadata": {},
   "source": [
    "# Steps Run the Streamlit Dashboard from a new Anaconda Terminal. Make a Prediction.\n",
    "\n",
    "1. Open a new Anaconda terminal.\n",
    "2. Browse to your `Lab Package directory` (as usual).\n",
    "3. You should find your two (2) newly exported files containing your model (.pkl).\n",
    "4. Type `cd dash` to enter into the Dashboard directory.\n",
    "5. If you do not not already have streamlit installed, then you can now `!pip install streamlit` (top of this file).\n",
    "6. In your terminal, type: `streamlit run app.py` this should run the web-based dashboard with your model.\n",
    "    * The command should open your default web browser.\n",
    "    * It should open the page at the URL for your dashboard on your localhost, e.g. http://localhost:8501\n",
    "    * If not, check the output messages for errors.\n",
    "    * *NB: Ctrl+C on the terminal will close the dashboard web service.*\n",
    "7. Make a `new prediction` at the bottom of the dashboard page.\n",
    "8. Open the `dash/predictions_data.csv` file in Excel to confirm your prediction and input was stored."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa37af0-8e85-4018-919b-4f5021dec60d",
   "metadata": {},
   "source": [
    "## Q10: Show your Prediction & Explain The Error of your Model to your LA\n",
    "1. Show `your input values` and  `your model's predicted value` *(Insert a screenshot if you like)*\n",
    "2. Show your saved prediction in `dash/predictions_data.csv` *(Insert a screenshot if you like)*\n",
    "3. Tell your LA - `how much average MAE error ($-AUD)` and `R2 score` is `attached to your model and its prediction`. (See your Q4). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a682ee6-daab-4088-8f32-beae755240ee",
   "metadata": {},
   "source": [
    "```\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "<p style=\"text-align:center;\">That's it! Congratulations! <br> \n",
    "    Now, call an LA to check your solution. Then, upload your code on MyCourses.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3c5c81-16fc-453a-a6e7-286dd9df39d9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "```\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "## **OPTIONAL EXTRA** - Partial Dependency Plots:\n",
    "### Intepreting Features on Price for our new model:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36ba843-3074-4f9f-807f-0b24731b14c6",
   "metadata": {},
   "source": [
    "- Like the `Residual Error Plots per Feature` we saw in the Tutorial, `Partial Dependency Plots` allow us see the average (independent) feature effect on `Y` in relative terms (i.e. relative house price change).\n",
    "\n",
    "Key points:\n",
    "\n",
    "> - PDP are to show an average of the marginal effects of a feature and  express the behaviour of the trained predictive model. [Hastie, 2009]\n",
    "> - The **primary assumption** for interpreting PDPs is that the features are independent (i.e. not influencing one-another).\n",
    "> - Note, this information of feature effects **cannot** be interpreted as effects in the underlying data or in the true population's behaviour. This is model interpretation, having parsed the dataset. That is different from Data Analysis, because the model has error: `f(x,p) = Y + error + noise`. Findings can be drawn from data analysis, specifically correlations (on the data sample).\n",
    "> \n",
    "> Reference: [H2009] T. Hastie, R. Tibshirani and J. Friedman, The Elements of Statistical Learning, Second Edition, Section 10.13.2, Springer, 2009.\n",
    "\n",
    "- Run the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051e192f-7e61-4d57-b373-4c737272eb08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import partial_dependence, PartialDependenceDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# features = ...\n",
    "\n",
    "fig, ax = plt.subplots(ncols=len(features), sharey=True, figsize=(16,4))\n",
    "_ = [a.axhline(y=0, color='r', linestyle='dashed') for a in ax] if isinstance(ax, np.ndarray) else ax.axhline(y=0, color='r', linestyle='dashed')\n",
    "PartialDependenceDisplay.from_estimator(my_model, \n",
    "                                   features=list(range(len(features))), \n",
    "                                   X=X_test, \n",
    "                                   feature_names=features, \n",
    "                                   # grid_resolution=10,\n",
    "                                   ax=ax\n",
    "                                       )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6011cab-24a3-4638-aff6-35d4b2f3c915",
   "metadata": {},
   "source": [
    "### Analysis of plots:\n",
    "- The blue line reports the effect of each variable's value on `Y`, which is the property/house `Price`.\n",
    "- The red dashed-line shows the intercept (crossing-point) at which the variable has no effect on the predicted `Y` value.\n",
    "- Falling and Rising trends:\n",
    "    - Falling, suggests a negative correlation with `Y`.\n",
    "    - Rising, suggests a positive correlation with `Y`.\n",
    "    - We can test this by normalizing the two variables and applying Pearson's Correlation Co-efficient (R) test. A rough version of that is below. The partial dependency are therefore consistent with the notion of statistical correlation.\n",
    "    - Here:\n",
    "        - We are analysing the `prediction model's representation of the true population`, as learnt from the dataset.\n",
    "        - We are not analysing the data sample of the true population (as we might do in Statistical Data Analysis)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3f7ed4-2d60-4f91-affa-27ae90a5d4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "\n",
    "def pearsons_r(a,b, alpha=0.05):\n",
    "    corr_coef, p_value = pearsonr(a,b)\n",
    "    print(f\"Pearson correlation coefficient: {corr_coef}\")\n",
    "    print(f\"P-value: {p_value}\")\n",
    "    print('Significantly' if p_value<alpha else 'Not significantly', 'Falling' if corr_coef<0 else 'Rising' if corr_coef>0 else 'Equal')\n",
    "dft = df.copy().dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4943375c-076c-45ea-95ea-8026cf3d0e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "pearsons_r( dft['Distance'] , dft['Price'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914818be-1f75-4e19-901b-fb74eea57558",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pearsons_r( dft['BuildingArea'] , dft['Price'] )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
